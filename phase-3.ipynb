{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7NljxdgmOa7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر سلیمانی\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>فاز سوم پروژه</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل:  ساعت ۶ صبح ۸ تیر<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fok75cDDmObO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h1>\n",
    "    <b>مقدمه</b>\n",
    "    </h1>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "    در این فاز از پروژه، تمرکز ما بر\n",
    "    crawling\n",
    "    و تحلیل مقالات استخراج‌شده از اینترنت خواهد بود. ما با بررسی تکنیک های مختلف  \n",
    "    web crawling\n",
    "    برای استخراج مقالات و سایر اطلاعات مرتبط از وب شروع خواهیم کرد.\n",
    "    <br>\n",
    "    در مرحله بعد، الگوریتم های تجزیه و تحلیل  لینک مانند\n",
    "    PageRank\n",
    "    و\n",
    "    HITS\n",
    "    را برای تعیین اهمیت این مقالات بر اساس نقل قول‌ها، ارجاعات یا اشکال دیگر پیوندها اعمال خواهیم‌کرد. ما همچنین یاد خواهیم‌گرفت که چگونه یک الگوریتم\n",
    "    PageRank\n",
    "    شخصی‌سازی‌شده را پیاده‌سازی کنیم که ترجیحات کاربر را برای ارائه نتایج مرتبط تر در نظر می‌گیرد.\n",
    "    <br>\n",
    "    در بخش سوم این مرحله، یک موتور جستجوی شخصی‌سازی شده را پیاده‌سازی میکنیم و یاد می‌گیریم که چگونه موتور جستجویی بسازیم که نتایجی را بر اساس ترجیحات کاربر ارائه دهد.\n",
    "    <br>\n",
    "در نهایت، ما یک\n",
    "    task\n",
    "     در مورد\n",
    "    recommendation system\n",
    "    ها خواهیم‌داشت، که در آن از تکنیک های مختلف برای توصیه مقالات یا صفحات وب به کاربران بر اساس ترجیحات و رفتار آنها استفاده خواهیم کرد.\n",
    "    <br>\n",
    "     تنها زبان قابل قبول برای پروژه پایتون است. محدودیت استفاده از کتاب‌خانه‌های آماده در هر بخش مشخص شده است. در انتهای پروژه قرار است یک سیستم یکپارچه‌ی جست‌و‌جو داشته باشید، بنابراین به پیاده‌سازی هر چه بهتر این فاز توجه داشته باشید.\n",
    "</font>\n",
    "</div>\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dmsAd0j_u73n",
    "outputId": "15ee8c9f-7849-488a-c852-e329302de0eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l17BkH7Hu73r",
    "outputId": "7646c484-073a-40c7-adf7-d870aa827fd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive\n"
     ]
    }
   ],
   "source": [
    "! cd /content/drive/MyDrive/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLzuuhAzmObS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>پیاده‌سازی Crawler (۴۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "   در این بخش باید یک Crawler\n",
    "    برای واکشی اطلاعات تعدادی مقاله از سایت <a href=\"https://www.semanticscholar.org/\">Semantic Scholar</a> پیاده سازی کنید.\n",
    "   اطلاعات واکشی شده باید حاوی موارد زیر باشد.\n",
    "</font>\n",
    "</div>\n",
    "<br>\n",
    "<table dir=\"ltr\" style=\"width: 100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">ID</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Title</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Abstract</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Publication Year</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Authors</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Related Topics</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Citation Count</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Reference Count</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">References</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Unique ID of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Title of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Abstract of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Publication year</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Name of the first author, ..., Name of the last author</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">topic1, topic2, ...</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">number of citations of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">number of references of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">ID of the first reference, ..., ID of the tenth reference</td>\n",
    "  </tr>\n",
    "</table>\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "0oy40jyZzrlt"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import heapq\n",
    "import json\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from collections import defaultdict, namedtuple\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import networkx as nx\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "8dFdsX99rcFv"
   },
   "outputs": [],
   "source": [
    "element = namedtuple('element', 'text')\n",
    "def find(soup, *args, **kwargs):\n",
    "    return soup.findAll(*args, **kwargs)\n",
    "\n",
    "def extract_title(soup):\n",
    "    title = find(soup, 'h1', attrs={'data-test-id': 'paper-detail-title'})[0].text\n",
    "    return title\n",
    "\n",
    "def extract_abstract(soup):\n",
    "    abstract_ = find(soup, 'span', attrs={'data-test-id': 'text-truncator-text'})\n",
    "    if not abstract_:\n",
    "        abstract_ = find(soup, 'div', attrs={'class': 'paper-detail__abstract__highlighted'}) # Highlighted\n",
    "        if not abstract_:\n",
    "            abstract_ = [element('')]\n",
    "\n",
    "    abstract = abstract_[0].text\n",
    "    return abstract\n",
    "\n",
    "def extract_year(soup):\n",
    "    year = find(soup, 'span', attrs={'data-test-id': 'paper-year'})[0].text[-4:]\n",
    "    return year\n",
    "\n",
    "def extract_authors(soup):\n",
    "    authors = [a.text for a in  find(soup, 'a', attrs={'class': 'author-list__link author-list__author-name'})]\n",
    "    return authors\n",
    "\n",
    "def extract_paper_related_topics(soup):\n",
    "    all_related_topics = []\n",
    "    paper_topics_ = find(soup, 'li',  attrs={'class': 'paper-meta-item'})\n",
    "    for topic in paper_topics_:\n",
    "        if not topic.find('span'):\n",
    "            all_related_topics = topic.text.split(',')\n",
    "\n",
    "    return all_related_topics\n",
    "\n",
    "\n",
    "def extract_refrences_related_topics(soup):\n",
    "    all_related_topics = []\n",
    "    refreneced_papers_topics_ = find(soup, 'span', attrs={'class': 'cl-paper-fos'})\n",
    "    for topics_list in refreneced_papers_topics_:\n",
    "        topics = topics_list.text.split(',') # just related topic doesn't have span\n",
    "        for topic in topics:\n",
    "            if topic not in all_related_topics:\n",
    "                all_related_topics.append(topic)\n",
    "    return all_related_topics\n",
    "\n",
    "\n",
    "def extract_citations_cnt(element):\n",
    "    for item in element:\n",
    "        if 'Citations' in item.text or 'Citation' in item.text:\n",
    "            count = item.text.split()[0]\n",
    "            if count == 'one' or count == 'One':\n",
    "                return 1\n",
    "            return  int(count.replace(',', '')) # Example: 65,000\n",
    "    return 0\n",
    "\n",
    "\n",
    "def extract_refrences_cnt(element):\n",
    "    for item in element:\n",
    "        if 'References' in item.text or 'Reference' in item.text:\n",
    "            count = item.text.split()[0]\n",
    "            if count == 'one' or count == 'One':\n",
    "                return 1\n",
    "            return int(count.replace(',', ''))\n",
    "\n",
    "    return 0\n",
    "\n",
    "def extract_paper_data(url, soup):\n",
    "    paper_id = url.split('/')[-1]\n",
    "    title = extract_title(soup)\n",
    "    abstract = extract_abstract(soup)\n",
    "    year = extract_year(soup)\n",
    "    authors = extract_authors(soup)\n",
    "\n",
    "    paper_related_topics = extract_paper_related_topics(soup)\n",
    "    refrences_related_topics = extract_refrences_related_topics(soup)\n",
    "    related_topics = list(set(paper_related_topics + refrences_related_topics))\n",
    "\n",
    "    citation_refrence_ = find(soup, 'span', attrs={'class': 'paper-nav__nav-label'})\n",
    "    citations_cnt = extract_citations_cnt(citation_refrence_)\n",
    "    references_cnt = extract_refrences_cnt(citation_refrence_)\n",
    "\n",
    "    refrences_urls_ = find(soup, 'a', attrs={'class': 'link-button--show-visited'})[:10] # First 10 papers are in the first page\n",
    "    references_urls = ['https://www.semanticscholar.org' + x['href'] for x in refrences_urls_] # Relative -> Absolute\n",
    "    references = [data['data-heap-paper-id'] for data in refrences_urls_] # Not used corpus Id \n",
    "\n",
    "    paper_data = {\n",
    "        'id': paper_id,\n",
    "        'title': title,\n",
    "        'abstract': abstract,\n",
    "        'year': year,\n",
    "        'authors': authors,\n",
    "        'related_topics': related_topics,\n",
    "        'citations_count': citations_cnt,\n",
    "        'references_count': references_cnt,\n",
    "        'references': references,\n",
    "    }\n",
    "    return paper_data, references_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NtUMM61FrcAw"
   },
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "def click(driver, button):\n",
    "    if len(driver.find_elements(By.XPATH, button)) > 0:\n",
    "        button = driver.find_element(By.XPATH, button)\n",
    "        try:\n",
    "            button.click()\n",
    "        except:\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].click();\", WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.XPATH, button))))\n",
    "            except:\n",
    "                return\n",
    "\n",
    "def crawl_paper(url):\n",
    "    '''\n",
    "        expand authors\n",
    "        expand abstracts\n",
    "        expands highlighted abstracts\n",
    "    '''\n",
    "    driver.get(url)\n",
    "    click(driver, '//*[@id=\"main-content\"]/div[1]/div/div/div[1]/div/div[1]/div/button')\n",
    "    click(driver, '//*[@id=\"main-content\"]/div[1]/div/div/div[1]/div/div[1]/div/div/span[2]/button')\n",
    "    click(driver, '//*[@id=\"main-content\"]/div[1]/div/div/div[1]/div/ul[2]/li[1]/span/span/span[3]/span[2]/button')\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    return extract_paper_data(url, soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yRox5s_Arb7s"
   },
   "outputs": [],
   "source": [
    "dr_soleymani_urls = [\n",
    "    'https://www.semanticscholar.org/paper/Transformer-based-deep-neural-network-language-for-Roshanzamir-Aghajan/6d8052588c62e5bf2a073ae414867a78784ff663',\n",
    "    'https://www.semanticscholar.org/paper/MG-BERT%3A-Multi-Graph-Augmented-BERT-for-Masked-BehnamGhader-Zakerinia/b9464b492f6638035d25b42f32ff3d51cb6d1e30',\n",
    "    'https://www.semanticscholar.org/paper/Deep-Learning-Based-Proarrhythmia-Analysis-Using-Golgooni-Mirsadeghi/b9e98f630e8eaf77ddcd0f80d1360b611ae61e70',\n",
    "    'https://www.semanticscholar.org/paper/A-Deep-Learning-Framework-for-Viable-Tumor-Burden-Jahromi-Khani/1eae26fe1ca566f17468080c3aecab1c3f9efb66',\n",
    "    'https://www.semanticscholar.org/paper/An-attribute-learning-method-for-zero-shot-Yazdanian-Shojaee/5a5f7d39433d68059e513b947a9fde62b5d4d3fe',\n",
    "]\n",
    "dr_kasae_urls = [\n",
    "    \"https://www.semanticscholar.org/paper/The-Eighth-Visual-Object-Tracking-VOT2020-Challenge-Kristan-Leonardis/12508951ba96b7d4c0906ed95542287d3ebdfd95\",\n",
    "    \"https://www.semanticscholar.org/paper/Benign-and-malignant-breast-tumors-classification-Rouhi-Jafari/c6db34ade32b3681a92068b22a354903b2953d52\",\n",
    "    \"https://www.semanticscholar.org/paper/Event-Detection-and-Summarization-in-Soccer-Videos-Tavassolipour-Karimian/ac2e54cec3aa2d1e67288d00c7fce7b7b17f9a73\",\n",
    "    \"https://www.semanticscholar.org/paper/An-efficient-PCA-based-color-transfer-method-Abadpour-Kasaei/53fc0415e0d00f9691994a49b8232a1cc2dfad5f\",\n",
    "    \"https://www.semanticscholar.org/paper/Deep-Learning-for-Visual-Tracking%3A-A-Comprehensive-Marvasti-Zadeh-Cheng/1fbb4201af091aef55360f113ba35814063923e4\",\n",
    "]\n",
    "\n",
    "dr_rabiee_urls = [\n",
    "    \"https://www.semanticscholar.org/paper/Spatial-Aware-Dictionary-Learning-for-Hyperspectral-Soltani-Farani-Rabiee/5ca94050fcf3382b50ec44629c0dda80c8843558\",\n",
    "    \"https://www.semanticscholar.org/paper/Multiresolution-Knowledge-Distillation-for-Anomaly-Salehi-Sadjadi/6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d\",\n",
    "    \"https://www.semanticscholar.org/paper/A-Hybrid-Deep-Learning-Architecture-for-Mobile-Osia-Shamsabadi/6f0685d61328f0f90972fe822258d574b74e9c7a\",\n",
    "    \"https://www.semanticscholar.org/paper/Novel-dataset-for-fine-grained-abnormal-behavior-in-Rabiee-Haddadnia/c626a9d75dfd73e26cf30793d5ef71527cd9fa95\",\n",
    "    \"https://www.semanticscholar.org/paper/Deep-Private-Feature-Extraction-Ossia-Taheri/e1dcd7fd049ae2ae3b93295d8ea360cafc00f9da\",\n",
    "]\n",
    "\n",
    "dr_rohban_urls = [\n",
    "    \"https://www.semanticscholar.org/paper/Nucleus-segmentation-across-imaging-experiments%3A-Caicedo-Goodman/0b5b33b7ea1dc12f3e9252ac1852170a6a6775bf\",\n",
    "    \"https://www.semanticscholar.org/paper/Multiresolution-Knowledge-Distillation-for-Anomaly-Salehi-Sadjadi/6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d\",\n",
    "    \"https://www.semanticscholar.org/paper/Data-analysis-strategies-for-image-based-cell-Caicedo-Cooper/1e1f905c5d8c6a2ad18b09ce8eb5d2b4c6b174f5\",\n",
    "    \"https://www.semanticscholar.org/paper/Minimax-Optimal-Sparse-Signal-Recovery-With-Poisson-Rohban-Saligrama/0e16a10560b4f7d5f891b932c79d1b2005407acf\",\n",
    "    \"https://www.semanticscholar.org/paper/ARAE%3A-Adversarially-Robust-Training-of-Autoencoders-Salehi-Arya/10b219619e88931fabb674037bbb633682775136\",\n",
    "]\n",
    "\n",
    "dr_sharifi_urls = [\n",
    "    \"https://www.semanticscholar.org/paper/Inhibition-of-TGF%CE%B2-Signaling-Promotes-Ground-State-Hassani-Totonchi/8d6835be604459b317fd2f26f9db4a6118586740\",\n",
    "    \"https://www.semanticscholar.org/paper/Treatment-of-human-embryonic-stem-cells-with-of-and-Tahamtani-Azarnia/788db0c4d5b98e835027cc2331419bf6b26f1c77\",\n",
    "    \"https://www.semanticscholar.org/paper/Graph-Traversal-Edit-Distance-and-Extensions-Boroojeny-Shrestha/0d095dcf4fb6e4cc701db75c7a2c1076440d3df8\",\n",
    "    \"https://www.semanticscholar.org/paper/DNA-methylation-regulates-discrimination-of-from-a-Sharifi-Zarchi-Gerovska/3056edcdfb86ec86bb8de369ce0f7fa0fae40e82\",\n",
    "    \"https://www.semanticscholar.org/paper/Downregulation-of-Extracellular-Matrix-and-Cell-in-Hassani-Oryan/a3f82efd05a3f6dbced63d4450d64eae5a4ebb97\",\n",
    "]\n",
    "\n",
    "\n",
    "crawl_config = {\n",
    "    \"SOLEYMANI\": {\n",
    "        'starting_urls': dr_soleymani_urls,\n",
    "        'queue_path': './soleymani_queue.pkl',\n",
    "        'data_path': './soleymani_data.pkl'\n",
    "    },\n",
    "    \"ROHBAN\": {\n",
    "        'starting_urls': dr_rohban_urls,\n",
    "        'queue_path': './rohban_queue.pkl',\n",
    "        'data_path': './rohban_data.pkl'\n",
    "    },\n",
    "    \"KASAEE\": {\n",
    "        'starting_urls': dr_kasae_urls,\n",
    "        'queue_path': './kasaee_queue.pkl',\n",
    "        'data_path': './kasaee_data.pkl'\n",
    "    },\n",
    "    \"RABIEE\": {\n",
    "        'starting_urls': dr_rabiee_urls,\n",
    "        'queue_path': './rabiee_queue.pkl',\n",
    "        'data_path': './rabiee_data.pkl'\n",
    "    },\n",
    "    \"SHARIFI\": {\n",
    "        'starting_urls': dr_sharifi_urls,\n",
    "        'queue_path': './sharifi_queue.pkl',\n",
    "        'data_path': './sharifi_data.pkl'\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdKGX88w2taN",
    "outputId": "6e30cd33-79ab-40fc-cb17-90acf7ba4d0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'id': '8d6835be604459b317fd2f26f9db4a6118586740',\n",
       "  'title': 'Inhibition of TGFβ Signaling Promotes Ground State Pluripotency',\n",
       "  'abstract': 'Embryonic stem (ES) cells are considered to exist in a ground state if shielded from differentiation triggers. Here we show that FGF4 and TGFβ signaling pathway inhibitors, designated R2i, not only provide the ground state pluripotency in production and maintenance of naïve ES cells from blastocysts of different mouse strains, but also maintain ES cells with higher genomic integrity following long-term cultivation compared with the chemical inhibition of the FGF4 and GSK3 pathways, known as 2i. Global transcriptome analysis of the ES cells highlights augmented BMP4 signaling pathway. The crucial role of the BMP4 pathway in maintaining the R2i ground state pluripotency is demonstrated by BMP4 receptor suppression, resulting in differentiation and cell death. In conclusion, by inhibiting TGFβ and FGF signaling pathways, we introduce a novel defined approach to efficiently establish the ground state pluripotency.\\xa0',\n",
       "  'year': '2014',\n",
       "  'authors': ['Seyedeh-Nafiseh Hassani',\n",
       "   'M. Totonchi',\n",
       "   'A. Sharifi-Zarchi',\n",
       "   'Sepideh Mollamohammadi',\n",
       "   'Mohammad Pakzad',\n",
       "   'S. Moradi',\n",
       "   'A. Samadian',\n",
       "   'N. Masoudi',\n",
       "   'Shahab Mirshahvaladi',\n",
       "   'A. Farrokhi',\n",
       "   'B. Greber',\n",
       "   'M. Araúzo-Bravo',\n",
       "   'D. Sabour',\n",
       "   'M. Sadeghi',\n",
       "   'G. Salekdeh',\n",
       "   'H. Gourabi',\n",
       "   'H. Schöler',\n",
       "   'H. Baharvand'],\n",
       "  'related_topics': ['Biology', ' Chemistry'],\n",
       "  'citations_count': 60,\n",
       "  'references_count': 40,\n",
       "  'references': ['0db10b4985f104f05220c4c22d227e5985ee183b',\n",
       "   '729c99f7ff22de34b5d361aeeb10fc215552e071',\n",
       "   '570e58a94931c5b8d8a73e058aa61236b7d9b7e0',\n",
       "   '9539b5f7f1601db56ec8af9b4531c5cdd0273208',\n",
       "   'cde4395e604a1ba5a952c49649323e8c5677ce1b',\n",
       "   '49416007a84a9dc715fe16aafccf17b40d46b04e',\n",
       "   'e2e65a21a7c246ec3c3b7edb0e873fd9a3d802fd',\n",
       "   '1b2a614bcd5dc8164a4466c144cbb6d129db3b1e',\n",
       "   'ba42c5b0fbe816cb47126d4633c35ba19ce9f018',\n",
       "   '96dc275e6afa706f4a2f1f492af2632a6f72be6f']},\n",
       " ['https://www.semanticscholar.org/paper/Genomic-integrity-of-ground%E2%80%90state-pluripotency-Jafari-Giehr/0db10b4985f104f05220c4c22d227e5985ee183b',\n",
       "  'https://www.semanticscholar.org/paper/Single-cell-RNA-sequencing-reveals-the-existence-of-Liu-Wang/729c99f7ff22de34b5d361aeeb10fc215552e071',\n",
       "  'https://www.semanticscholar.org/paper/miR-302b-3p-Promotes-Self-Renewal-Properties-in-Moradi-Braun/570e58a94931c5b8d8a73e058aa61236b7d9b7e0',\n",
       "  'https://www.semanticscholar.org/paper/Blockage-of-the-Epithelial-to-Mesenchymal-Is-for-Totonchi-Hassani/9539b5f7f1601db56ec8af9b4531c5cdd0273208',\n",
       "  'https://www.semanticscholar.org/paper/Efficient-induction-of-pluripotency-in-primordial-Attari-Sepehri/cde4395e604a1ba5a952c49649323e8c5677ce1b',\n",
       "  'https://www.semanticscholar.org/paper/Temporal-Gene-Expression-and-DNA-Methylation-during-Samadian-Hesaraki/49416007a84a9dc715fe16aafccf17b40d46b04e',\n",
       "  'https://www.semanticscholar.org/paper/Small-RNA-Sequencing-Reveals-Dlk1-Dio3-MicroRNAs-as-Moradi-Sharifi-Zarchi/e2e65a21a7c246ec3c3b7edb0e873fd9a3d802fd',\n",
       "  'https://www.semanticscholar.org/paper/From-iPSC-towards-cardiac-tissue%E2%80%94a-road-under-Peischard-Piccini/1b2a614bcd5dc8164a4466c144cbb6d129db3b1e',\n",
       "  'https://www.semanticscholar.org/paper/Application-Of-Small-Molecules-Favoring-Na%C3%AFve-Human-JeughtMargot-TaelmanJasin/ba42c5b0fbe816cb47126d4633c35ba19ce9f018',\n",
       "  'https://www.semanticscholar.org/paper/Inhibition-of-TGF-%CE%B2-pathway-improved-the-of-porcine-Gao-Wu/96dc275e6afa706f4a2f1f492af2632a6f72be6f'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawl_paper(dr_sharifi_urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7nd9Y95Xj7da"
   },
   "outputs": [],
   "source": [
    "def save(data, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load(path):\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "\n",
    "def is_paper_repeated(paper_data, data):\n",
    "    for datum in data:\n",
    "        if datum['id'] == paper_data['id']:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def crawl_professor_data(starting_urls, middle_path, output_path):\n",
    "\n",
    "    '''\n",
    "        starting_urls: an array of urls.\n",
    "        middle_path: a temporary path for queue.\n",
    "        output_path: path of the craweled data.\n",
    "\n",
    "    '''\n",
    "    bulk_read_size, depth_size = 25, 20\n",
    "    queue, data = starting_urls, []\n",
    "    save(queue, middle_path)\n",
    "    save(data, output_path)\n",
    "    for d in range(depth_size):\n",
    "        queue = load(middle_path)\n",
    "        data = load(output_path)\n",
    "        for w in range(bulk_read_size):\n",
    "            try:\n",
    "                url = queue.pop(0)\n",
    "                time.sleep(1)\n",
    "                paper_data, refrences_urls = crawl_paper(url)\n",
    "                print(paper_data, refrences_urls)\n",
    "                if is_paper_repeated(paper_data, data):\n",
    "                    continue\n",
    "                data.append(paper_data)\n",
    "                for r_url in refrences_urls:\n",
    "                    if r_url not in queue:\n",
    "                        queue.append(r_url)\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        save(queue, middle_path)\n",
    "        save(data, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "UKC2Kckg1aqX",
    "outputId": "ec23916f-2f04-47d8-b2ed-52401513cdfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '6d8052588c62e5bf2a073ae414867a78784ff663', 'title': 'Transformer-based deep neural network language models for Alzheimer’s disease risk assessment from targeted speech', 'abstract': 'Background We developed transformer-based deep learning models based on natural language processing for early risk assessment of Alzheimer’s disease from the picture description test. Methods The lack of large datasets poses the most important limitation for using complex models that do not require feature engineering. Transformer-based pre-trained deep language models have recently made a large leap in NLP research and application. These models are pre-trained on available large datasets to understand natural language texts appropriately, and are shown to subsequently perform well on classification tasks with small training sets. The overall classification model is a simple classifier on top of the pre-trained deep language model. Results The models are evaluated on picture description test transcripts of the Pitt corpus, which contains data of 170 AD patients with 257 interviews and 99 healthy controls with 243 interviews. The large bidirectional encoder representations from transformers (BERT Large ) embedding with logistic regression classifier achieves classification accuracy of 88.08%, which improves the state-of-the-art by 2.48%. Conclusions Using pre-trained language models can improve AD prediction. This not only solves the problem of lack of sufficiently large datasets, but also reduces the need for expert-defined features.\\xa0', 'year': '2021', 'authors': ['A. Roshanzamir', 'H. Aghajan', 'Mahdieh Soleymani Baghshah'], 'related_topics': ['Biology', 'Psychology', 'Computer Science'], 'citations_count': 32, 'references_count': 54, 'references': ['623c9b5574306cb58c9ec20332726c0242bb8667', 'eec962309a9b3bbae2740045820a8df0f8cad13c', 'b5fdd83f8235655c41c784769197e2b25598fcca', '72b3390486d9b9e4f520e158eae290219d68fc16', '915a55642ba2930061f625b86fee7daa362769cf', '9358d9e9afbc1eaf6b2f2042a8adc573556f566e', 'bb583b71b6a7f09a41e5a2840c16fae0dff325e7', '4884e4edb5268c065cbc191a65eadde172d66bbf', '7d96e0fd04544cdcf73f2c34504bd7784cbc7b28', '84599e152d96b9c33cd216339e8d2a600111a660']} ['https://www.semanticscholar.org/paper/A-Transfer-Learning-Method-for-Detecting-Disease-on-Liu-Luo/623c9b5574306cb58c9ec20332726c0242bb8667', \"https://www.semanticscholar.org/paper/Improving-Alzheimer's-Disease-Detection-for-Speech-Liu-Yuan/eec962309a9b3bbae2740045820a8df0f8cad13c\", 'https://www.semanticscholar.org/paper/GPT-D%3A-Inducing-Dementia-related-Linguistic-by-of-Li-Knopman/b5fdd83f8235655c41c784769197e2b25598fcca', 'https://www.semanticscholar.org/paper/Deep-learning-based-speech-analysis-for-Alzheimer%E2%80%99s-Yang-Li/72b3390486d9b9e4f520e158eae290219d68fc16', \"https://www.semanticscholar.org/paper/Data-Augmentation-for-Dementia-Detection-in-Spoken-Hl'edikov'a-Woszczyk/915a55642ba2930061f625b86fee7daa362769cf\", 'https://www.semanticscholar.org/paper/Comparative-study-of-Deep-Classifiers-for-Early-Nambiar-Likhita/9358d9e9afbc1eaf6b2f2042a8adc573556f566e', \"https://www.semanticscholar.org/paper/Learning-implicit-sentiments-in-Alzheimer's-disease-Liu-Yuan/bb583b71b6a7f09a41e5a2840c16fae0dff325e7\", 'https://www.semanticscholar.org/paper/OViTAD%3A-Optimized-Vision-Transformer-to-Predict-of-Sarraf-Sarraf/4884e4edb5268c065cbc191a65eadde172d66bbf', 'https://www.semanticscholar.org/paper/Deep-Learning-Based-Diagnosis-of-Alzheimer%E2%80%99s-Saleem-Zahra/7d96e0fd04544cdcf73f2c34504bd7784cbc7b28', 'https://www.semanticscholar.org/paper/A-Cross-language-Dementia-Classifier%3A-a-Preliminary-Bertini-Allevi/84599e152d96b9c33cd216339e8d2a600111a660']\n",
      "{'id': 'b9464b492f6638035d25b42f32ff3d51cb6d1e30', 'title': 'MG-BERT: Multi-Graph Augmented BERT for Masked Language Modeling', 'abstract': 'Pre-trained models like Bidirectional Encoder Representations from Transformers (BERT), have recently made a big leap forward in Natural Language Processing (NLP) tasks. However, there are still some shortcomings in the Masked Language Modeling (MLM) task performed by these models. In this paper, we first introduce a multi-graph including different types of relations between words. Then, we propose Multi-Graph augmented BERT (MG-BERT) model that is based on BERT. MG-BERT embeds tokens while taking advantage of a static multi-graph containing global word co-occurrences in the text corpus beside global real-world facts about words in knowledge graphs. The proposed model also employs a dynamic sentence graph to capture local context effectively. Experimental results demonstrate that our model can considerably enhance the performance in the MLM task.\\xa0', 'year': '2021', 'authors': ['Parishad BehnamGhader', 'Hossein Zakerinia', 'Mahdieh Soleymani Baghshah'], 'related_topics': ['Computer Science'], 'citations_count': 1, 'references_count': 29, 'references': ['44a422a2514c1cd6828423b5edce53d0dbdabd73', '5f994dc8cae24ca9d1ed629e517fcc652660ddde', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', '31184789ef4c3084af930b1e0dede3215b4a9240', 'd6a13d8d168936a8947101d76fe060704d2f26ec', 'b36b2914f16c78b1bf88ee720342d893d8a9fc46', '56cafbac34f2bb3f6a9828cd228ff281b810d6bb', 'cd8a9914d50b0ac63315872530274d158d6aff09', '2582ab7c70c9e7fcb84545944eba8f3a7f253248', '2cab7f5d64a427cb59fb21112fe8dc28fb753b56']} ['https://www.semanticscholar.org/paper/Structure-inducing-pre-training-McDermott-Yap/44a422a2514c1cd6828423b5edce53d0dbdabd73', 'https://www.semanticscholar.org/paper/ERNIE%3A-Enhanced-Language-Representation-with-Zhang-Han/5f994dc8cae24ca9d1ed629e517fcc652660ddde', 'https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992', 'https://www.semanticscholar.org/paper/KG-BERT%3A-BERT-for-Knowledge-Graph-Completion-Yao-Mao/31184789ef4c3084af930b1e0dede3215b4a9240', 'https://www.semanticscholar.org/paper/Barack%E2%80%99s-Wife-Hillary%3A-Using-Knowledge-Graphs-for-RobertL.Logan-Liu/d6a13d8d168936a8947101d76fe060704d2f26ec', 'https://www.semanticscholar.org/paper/Learning-beyond-Datasets%3A-Knowledge-Graph-Augmented-Annervaz-Chowdhury/b36b2914f16c78b1bf88ee720342d893d8a9fc46', 'https://www.semanticscholar.org/paper/KEPLER%3A-A-Unified-Model-for-Knowledge-Embedding-and-Wang-Gao/56cafbac34f2bb3f6a9828cd228ff281b810d6bb', 'https://www.semanticscholar.org/paper/Modeling-Relational-Data-with-Graph-Convolutional-Schlichtkrull-Kipf/cd8a9914d50b0ac63315872530274d158d6aff09', 'https://www.semanticscholar.org/paper/Translating-Embeddings-for-Modeling-Data-Bordes-Usunier/2582ab7c70c9e7fcb84545944eba8f3a7f253248', 'https://www.semanticscholar.org/paper/Enriching-BERT-with-Knowledge-Graph-Embeddings-for-Ostendorff-Bourgonje/2cab7f5d64a427cb59fb21112fe8dc28fb753b56']\n",
      "{'id': 'b9e98f630e8eaf77ddcd0f80d1360b611ae61e70', 'title': 'Deep Learning-Based Proarrhythmia Analysis Using Field Potentials Recorded From Human Pluripotent Stem Cells Derived Cardiomyocytes', 'abstract': 'An early characterization of drug-induced cardiotoxicity may be possible by combining comprehensive in vitro proarrhythmia assay and deep learning techniques. We aimed to develop a method to automatically detect irregular beating rhythm of field potentials recorded from human pluripotent stem cells (hPSC) derived cardiomyocytes (hPSC-CM) by multi-electrode array (MEA) system. We included field potentials from 380 experiments, which were labeled as normal or arrhythmic by electrophysiology experts. Convolutional and recurrent neural networks (CNN and RNN) were employed for automatic classification of field potential recordings. A preparation phase was initially applied to split 60-s long recordings into a series of 5-s windows. Subsequently, the classification phase comprising of two main steps was designed and applied. The first step included the classification of 5-s windows by using a designated CNN. While, the results of 5-s window assessments were used as the input sequence to an RNN that aggregates these results in the second step. The output was then compared to electrophysiologist-level arrhythmia detection, resulting in 0.83 accuracy, 0.93 sensitivity, 0.70 specificity, and 0.80 precision. In summary, this paper introduces a novel method for automated analysis of “irregularity” in an in vitro model of cardiotoxicity experiments. Thus, our method may overcome the drawbacks of using predesigned features that restricts the classification performance to the comprehensiveness and the quality of the designed features. Furthermore, automated analysis may facilitate the quality control experiments through the procedure of drug development with respect to cardiotoxicity and avoid late drug attrition from market.\\xa0', 'year': '2019', 'authors': ['Zeinab Golgooni', 'Sara Mirsadeghi', 'Mahdieh Soleymani Baghshah', 'P. Ataee', 'H. Baharvand', 'S. Pahlavan', 'H. Rabiee'], 'related_topics': [' Medicine', 'Biology', 'Computer Science', 'Medicine', ' Biology'], 'citations_count': 8, 'references_count': 46, 'references': ['447afc6231eb05eb43040d1eedcc4ce8fb83dbdb', '258a795ac1ab24c6aefb6cb13b8ebb11bc191f53', '45016f9aae59e7dbafcc7dd3c4dc09e81eab2f77', '2f50b9125e74a2d9b02dff9787fc9b73320a2a4a', '770484617f32062b38a450857bade1bb08c50b59', '1959a6d5627be11afcabd21797627e0894a0d05c', '8a73d44ab7a59d76f97b629a43b5eb4c18960967', '20484b42bd462ef016de99ffa71f67c6c295ed8a', '50afa4fa74b0475ca0264461c79f7bd42fcc494c', '307ff8f512098497e2c69b79c00fbb7b3cc9650e']} ['https://www.semanticscholar.org/paper/A-deep-learning-platform-to-assess-drug-risk.-Serrano-Feyen/447afc6231eb05eb43040d1eedcc4ce8fb83dbdb', 'https://www.semanticscholar.org/paper/Machine-Learning-Techniques-to-Classify-Healthy-and-Teles-Kim/258a795ac1ab24c6aefb6cb13b8ebb11bc191f53', 'https://www.semanticscholar.org/paper/Assessment-of-Drug-Proarrhythmicity-Using-Neural-in-Yoo-Marcellinus/45016f9aae59e7dbafcc7dd3c4dc09e81eab2f77', 'https://www.semanticscholar.org/paper/Characterizing-arrhythmia-using-machine-learning-of-Pang-Chia/2f50b9125e74a2d9b02dff9787fc9b73320a2a4a', 'https://www.semanticscholar.org/paper/Human-Induced-Pluripotent-Stem-Cell-Reprogramming-Chang-Abe/770484617f32062b38a450857bade1bb08c50b59', 'https://www.semanticscholar.org/paper/From-biomimicry-to-bioelectronics%3A-Smart-materials-Bolonduro-Duffy/1959a6d5627be11afcabd21797627e0894a0d05c', 'https://www.semanticscholar.org/paper/Ensemble-Deep-Learning-Models-for-Heart-Disease-A-Baccouche-Garcia-Zapirain/8a73d44ab7a59d76f97b629a43b5eb4c18960967', 'https://www.semanticscholar.org/paper/Analysis-of-the-Drawbacks-of-English-Chinese-Based-Li-Xiong/20484b42bd462ef016de99ffa71f67c6c295ed8a', 'https://www.semanticscholar.org/paper/In-vitro-electrophysiological-drug-testing-using-Caspi-Itzhaki/50afa4fa74b0475ca0264461c79f7bd42fcc494c', 'https://www.semanticscholar.org/paper/Estimating-the-risk-of-drug-induced-proarrhythmia-Guo-Abrams/307ff8f512098497e2c69b79c00fbb7b3cc9650e']\n",
      "{'id': '1eae26fe1ca566f17468080c3aecab1c3f9efb66', 'title': 'A Deep Learning Framework for Viable Tumor Burden Estimation', 'abstract': 'Liver masses have become a common clinical challenge since they require to be defined and accurately categorized as neoplastic or nonneoplastic lesions. Hepatocellular carcinoma (HCC), the most common histologic type of primary liver malignancy, is a global health concern being the fifth most common cancer and the second cause of cancer mortality worldwide. Accurate diagnosis, which in some circumstances requires histopathology results, is necessary for appropriate management. Also, some tumor characteristics help in predicting tumor behavior and patient response to therapy. In this paper, we propose a deep learning framework for the segmentation of whole and viable tumor areas of liver cancer from whole-slide images (WSIs). To this end, we use Fast Segmentation Convolutional Neural Network (Fast-SCNN) as our network. We use the dataset from PAIP 2019 challenge. After data-augmentation on the training subset, we train the network with a multi-term loss function and SWA technique. Our model achieves 0.80 for the median of the Jaccard Index for the task of Viable Tumor Segmentation and 0.77 for the median of Weighted Absolute Accuracy for the task of Viable Tumor Burden Estimation on the whole-slide images of the test subset.\\xa0', 'year': '2020', 'authors': ['Seyed Alireza Fatemi Jahromi', 'Ali Asghar Khani', 'Hatef Otroshi Shahreza', 'Mahdieh Soleymani Baghshah', 'H. Behroozi'], 'related_topics': ['Medicine', ' Medicine', 'Biology', 'Computer Science'], 'citations_count': 2, 'references_count': 62, 'references': ['9dbdcc1701aec174a6d0cf748b9bf5704ee4640a', 'd43f2a1cefcd8a508c7f7dafd1fffdf01dd69394', 'd779b87172306c37c2c711512e84bc8112adf21e', '915adc7d9aacc46b6b8575f4a8be4b7cb4a1caf7', '769149c0dc0ed308eca8bc916f4326b2e2f57a1f', '21ba757bf394720e0b66b86e7638ae28742d6570', '47a0dd130fbf397c554cfcbfdedda121c017c4ca', '6048de9749a1f31ac70e5c30030ceb1dc5d3f2b0', 'ba913e2c03ece1c75f0af4d16dd11c7ffbc6e3ba', '188f8f6f70947215a9dfeebb0b577155e0d3d339']} ['https://www.semanticscholar.org/paper/Structure-aware-scale-adaptive-networks-for-cancer-Sun-Molina/9dbdcc1701aec174a6d0cf748b9bf5704ee4640a', 'https://www.semanticscholar.org/paper/A-CNN-Based-Wearable-Assistive-System-for-Visually-Hsieh-Cheng/d43f2a1cefcd8a508c7f7dafd1fffdf01dd69394', 'https://www.semanticscholar.org/paper/Towards-Automatic-Prostate-Gleason-Grading-Via-Deep-Khani-Jahromi/d779b87172306c37c2c711512e84bc8112adf21e', 'https://www.semanticscholar.org/paper/Detecting-Cancer-Metastases-on-Gigapixel-Pathology-Liu-Gadepalli/915adc7d9aacc46b6b8575f4a8be4b7cb4a1caf7', 'https://www.semanticscholar.org/paper/Classification-and-mutation-prediction-from-cell-Coudray-Ocampo/769149c0dc0ed308eca8bc916f4326b2e2f57a1f', 'https://www.semanticscholar.org/paper/Deep-Learning-for-Identifying-Metastatic-Breast-Wang-Khosla/21ba757bf394720e0b66b86e7638ae28742d6570', 'https://www.semanticscholar.org/paper/Multi-scale-fully-convolutional-neural-networks-for-Schmitz-Madesta/47a0dd130fbf397c554cfcbfdedda121c017c4ca', 'https://www.semanticscholar.org/paper/PFA-ScanNet%3A-Pyramidal-Feature-Aggregation-with-for-Zhao-Lin/6048de9749a1f31ac70e5c30030ceb1dc5d3f2b0', 'https://www.semanticscholar.org/paper/Diagnostic-Assessment-of-Deep-Learning-Algorithms-Bejnordi-Veta/ba913e2c03ece1c75f0af4d16dd11c7ffbc6e3ba', 'https://www.semanticscholar.org/paper/1399-H%26E-stained-sentinel-lymph-node-sections-of-Litjens-B%C3%A1ndi/188f8f6f70947215a9dfeebb0b577155e0d3d339']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-189b675eb0c3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrawl_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstarting_urls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'starting_urls'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'queue_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcrawl_professor_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstarting_urls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-91be18d104d2>\u001b[0m in \u001b[0;36mcrawl_professor_data\u001b[0;34m(starting_urls, middle_path, output_path)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mpaper_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefrences_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrawl_paper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefrences_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_paper_repeated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-12a2817ae6e4>\u001b[0m in \u001b[0;36mcrawl_paper\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcrawl_paper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'//*[@id=\"main-content\"]/div[1]/div/div/div[1]/div/div[1]/div/button'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'//*[@id=\"main-content\"]/div[1]/div/div/div[1]/div/div[1]/div/div/span[2]/button'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;34m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    342\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sessionId\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/remote_connection.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{self._url}{path}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/remote_connection.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_alive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0mstatuscode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/request.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     76\u001b[0m             )\n\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             return self.request_encode_body(\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/request.py\u001b[0m in \u001b[0;36mrequest_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mextra_kw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/poolmanager.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0mredirect_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mredirect\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_redirect_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    715\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    464\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    459\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for key, value in crawl_config.items():\n",
    "    starting_urls, queue_path, data_path = value['starting_urls'], value['queue_path'], value['data_path']\n",
    "    crawl_professor_data(starting_urls, queue_path, data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "KMIJkupF3zrf"
   },
   "outputs": [],
   "source": [
    "def shuffle_list(my_list):\n",
    "    for i in range(len(my_list) - 1, 0, -1):\n",
    "        j = random.randint(0, i)\n",
    "        my_list[i], my_list[j] = my_list[j], my_list[i]\n",
    "\n",
    "\n",
    "soleymani_data = load(crawl_config['SOLEYMANI']['data_path'])\n",
    "rohban_data = load(crawl_config['ROHBAN']['data_path'])\n",
    "rabiee_data = load(crawl_config['RABIEE']['data_path'])\n",
    "kasaee_data = load(crawl_config['KASAEE']['data_path'])\n",
    "sharifi_data = load(crawl_config['SHARIFI']['data_path'])\n",
    "data = soleymani_data + rohban_data + rabiee_data + kasaee_data + sharifi_data\n",
    "shuffle_list(data)\n",
    "data = data[:2000]\n",
    "save(data, 'dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "txDbxwvzTlP2"
   },
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'SOLEYMANI': soleymani_data,\n",
    "    'ROHBAN': rohban_data,\n",
    "    'RABIEE': rabiee_data,\n",
    "    'KASAEE': kasaee_data,\n",
    "    'SHARIFI': sharifi_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53TiFEvHmOba",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "  ابتدا فرایند واکشی را از ۵ مقاله‌ی هر استاد شروع کنید و\n",
    "    ۱۰\n",
    "    مرجع اول هر مقاله را به صف مقالات اضافه کنید.\n",
    "    فرایند واکشی را نا جایی ادامه دهید که اطلاعات ۲۰۰۰ مقاله را داشته باشید.\n",
    "    اطلاعات مقالات را در فایل crawled_paper_profName.json ذخیره کنید.\n",
    "</font>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "  در پیاده سازی Crawler به موارد زیر دقت کنید.\n",
    "    \n",
    "    \n",
    "<ul>\n",
    "<li>حق استفاده از api سایت semantic scholar را ندارید.</li>\n",
    "<li>برای واکشی می‌توانید از پکیج‌هایی مثل <a href=\"https://www.selenium.dev/selenium/docs/api/py/\">Selenium</a> و یا <a href=\"https://github.com/scrapy/scrapy\">Scrapy</a>  استفاده کنید. استفاده از پکیج‌های دیگر نیز مجاز است. همچنین برای پارس اطلاعات واکشی شده می‌توانید از پکیج <a href=\"https://pypi.org/project/beautifulsoup4/\">Beautiful Soup</a> استفاده کنید.\n",
    "</li>\n",
    "<li>بین هر بار درخواست از سایت یک فاصله چند ثانیه‌ای بدهید.</li>\n",
    "<li>در زمان تحویل کد Crawler شما اجرا خواهد شد و صحت آن بررسی خواهد شد.</li>\n",
    "<li>در صورتی که ‌Crawler شما به دچار اروری مثل request timeout شد نباید کار خود را متوقف کند.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9rIo4admObc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>PageRank\n",
    "        شخصی‌سازی‌شده\n",
    "        (۲۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این بخش، الگوریتم\n",
    "    PageRank\n",
    "    شخصی‌سازی‌شده را پیاده‌سازی می‌کنیم که توسعه‌ای از الگوریتم\n",
    "    PageRank\n",
    "    است که ترجیحات کاربر را در نظر می‌گیرد. الگوریتم\n",
    "    PageRank\n",
    "    شخصی‌سازی‌شده گره‌ها را در یک گراف بر اساس اهمیت آنها برای کاربر رتبه‌بندی می‌کند، نه بر اساس اهمیت کلی آنها در نمودار.\n",
    "    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "M9n1N8-TmObe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pagerank(graph: Dict[str, List[str]]) -> Dict[str, float]:\n",
    "    nodes = list(graph.keys())\n",
    "    n = len(nodes)\n",
    "    A = np.array([1 / n] * n)\n",
    "    P = np.zeros((n, n))\n",
    "\n",
    "    #initialize pr\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if nodes[j] in graph.get(nodes[i], []):\n",
    "                P[i, j] = 1\n",
    "\n",
    "    default_value = np.array([1 / n] * n)\n",
    "    for i in range(n):\n",
    "        P[i] = np.array(default_value if sum(P[i]) == 0 else P[i] / sum(P[i]))\n",
    "\n",
    "    P = 0.9 * P + 0.1 * np.ones((n, n)) / n\n",
    "    w, v = np.linalg.eig(P.T)\n",
    "    idx = np.argmin(abs(w - 1.0))\n",
    "    steady_state = v[:, idx].real\n",
    "    steady_state /= np.sum(steady_state)\n",
    "    return {node: steady_state[index] for index, node in enumerate(nodes)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-I7QBRTmObj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این بخش از الگوریتم\n",
    "PageRank\n",
    "شخصی‌سازی‌شده که در قسمت قبلی پیاده‌سازی شده‌است برای\n",
    "شناسایی مقالات مهم مرتبط با حوزه‌ی کاری یک استاد\n",
    "خاص استفاده می‌کنیم. این تابع، یک\n",
    "    field\n",
    "    را به عنوان ورودی دریافت می‌کند. خروجی نیز\n",
    "مقالات برتری که بیشترین ارتباط را با آن زمینه دارند؛ خواهدبود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "ZM5ZNWp3mObl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def important_articles(Professor: str) -> List[str]:\n",
    "    data = data_dict.get(Professor, [])\n",
    "    graph = {paper['id']: paper['references'] for paper in data}\n",
    "    papers = pagerank(graph)\n",
    "    return heapq.nlargest(10, papers, key=papers.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ITzhBrtRjT2",
    "outputId": "261c17fa-9f99-4a62-8bff-ad5d741465ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['72b3390486d9b9e4f520e158eae290219d68fc16',\n",
       " 'ba913e2c03ece1c75f0af4d16dd11c7ffbc6e3ba',\n",
       " 'f7ffc466251ae8b781fa49eab913edb5f3b76765',\n",
       " '4884e4edb5268c065cbc191a65eadde172d66bbf',\n",
       " '5fd80e47d53c64512a0b85a4c7a0beb24bc35766',\n",
       " 'df6103a88acf71bf60e98b77421c58cbf243c5e2',\n",
       " '2a4822e63a052adf39b5c7211d9b67559c1b031f',\n",
       " '8a73d44ab7a59d76f97b629a43b5eb4c18960967',\n",
       " '9358d9e9afbc1eaf6b2f2042a8adc573556f566e',\n",
       " '194c36d79c7af11254637cfaa5cc54389ae7ab9b']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_articles('SOLEYMANI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yk7WfuirmObo",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>جستجو شخصی‌سازی‌شده (۱۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "الگوریتم جست‌و‌جویی که در فازهای گذشته پیاده‌سازی کرده‌اید را به گونه‌ای تغییر دهید که نتایج به دست آمده جست‌و‌جو بر حسب علایق فرد مرتب شوند. از قضیه‌ی خطی بودن برای این کار استفاده کنید.\n",
    "    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VI_NUHT9bE9s",
    "outputId": "4b866fb2-e410-4f1d-a6fd-e5a1de98b821"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ENWxDbYviwwp"
   },
   "outputs": [],
   "source": [
    "def extract_mapping_from_paper_id_to_prof():\n",
    "    mapping = {}\n",
    "    for prof_name, prof_data in data_dict.items():\n",
    "        for data in prof_data:\n",
    "            id = data['id']\n",
    "            mapping[id] = prof_name\n",
    "\n",
    "    return mapping\n",
    "\n",
    "id_prof_mapping = extract_mapping_from_paper_id_to_prof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "vT0Yw2mjdLjW",
    "outputId": "a312a923-362e-4337-8280-f01e4ed289a1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-41327d13-b528-4483-83c3-8c3893a76edb\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Protein function prediction using frequent pat...</td>\n",
       "      <td>Protein function prediction is one of the most...</td>\n",
       "      <td>773dafb377c6c2340b231522c43563380ff5c622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Automated Analysis of Ultrasound Videos for De...</td>\n",
       "      <td>Background: Breast cancer is the second cause ...</td>\n",
       "      <td>5d5f2f44fa6f251f3a663b0be9abf09a75772ec9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNet++: A Nested U-Net Architecture for Medica...</td>\n",
       "      <td>In this paper, we present UNet++, a new, more ...</td>\n",
       "      <td>a6876ea89e677a7cc42dd43f27165ff6fd414de5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Multi-omics insights and therapeutic implicati...</td>\n",
       "      <td>Polycystic ovary syndrome (PCOS) is a common g...</td>\n",
       "      <td>1275f8fdcd2fbf24c9bbad2aafd4a753ad586551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>An anatomy-based iteratively searching convolu...</td>\n",
       "      <td>Organ localization is a common and essential p...</td>\n",
       "      <td>9c8d162fb3558c3c521435fb1eb5502deb8c55fd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41327d13-b528-4483-83c3-8c3893a76edb')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-41327d13-b528-4483-83c3-8c3893a76edb button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-41327d13-b528-4483-83c3-8c3893a76edb');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                              titles  \\\n",
       "0  Protein function prediction using frequent pat...   \n",
       "1  Automated Analysis of Ultrasound Videos for De...   \n",
       "2  UNet++: A Nested U-Net Architecture for Medica...   \n",
       "3  Multi-omics insights and therapeutic implicati...   \n",
       "4  An anatomy-based iteratively searching convolu...   \n",
       "\n",
       "                                           abstracts  \\\n",
       "0  Protein function prediction is one of the most...   \n",
       "1  Background: Breast cancer is the second cause ...   \n",
       "2  In this paper, we present UNet++, a new, more ...   \n",
       "3  Polycystic ovary syndrome (PCOS) is a common g...   \n",
       "4  Organ localization is a common and essential p...   \n",
       "\n",
       "                                         id  \n",
       "0  773dafb377c6c2340b231522c43563380ff5c622  \n",
       "1  5d5f2f44fa6f251f3a663b0be9abf09a75772ec9  \n",
       "2  a6876ea89e677a7cc42dd43f27165ff6fd414de5  \n",
       "3  1275f8fdcd2fbf24c9bbad2aafd4a753ad586551  \n",
       "4  9c8d162fb3558c3c521435fb1eb5502deb8c55fd  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = [d[\"title\"] for d in data]\n",
    "ids = [d['id'] for d in data]\n",
    "abstracts = [d[\"abstract\"] for d in data]\n",
    "pdf = pd.DataFrame({'titles': titles, 'abstracts': abstracts, 'id': ids})\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "aYmeQq5HmObq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text, minimum_length=1, stopword_removal=True, stopwords_domain=[], lower_case=True,\n",
    "                       punctuation_removal=True):\n",
    "    tokens = word_tokenize(text)\n",
    "    if stopword_removal:\n",
    "        stopwords = [x.lower() for x in nltk.corpus.stopwords.words('english')]\n",
    "        domain_stopwords = [x.lower() for x in stopwords_domain]\n",
    "        tokens = [word for word in tokens if word.lower() not in domain_stopwords + stopwords]\n",
    "\n",
    "    if punctuation_removal:\n",
    "        tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    if lower_case:\n",
    "        tokens = [word.lower() for word in tokens if len(word) > minimum_length]\n",
    "    else:\n",
    "        tokens = [word for word in tokens if len(word) > minimum_length]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "corpus = []\n",
    "for i, (index, row) in enumerate(pdf.iterrows()):\n",
    "    titles, summaries, id = row['titles'], row['abstracts'], row['id']\n",
    "    title_token = preprocess_text(titles)\n",
    "    summaries_token = preprocess_text(summaries)\n",
    "    corpus.append({'id': id, 'title':title_token, 'abstract':summaries_token})\n",
    "\n",
    "title_lengths = defaultdict(int)\n",
    "summaries_lengths = defaultdict(int)\n",
    "for data in corpus:\n",
    "    title_tokens, summaries_tokens, paper_id = data['title'], data['abstract'], data['id']\n",
    "    title_lengths[paper_id] = len(title_tokens)\n",
    "    summaries_lengths[paper_id] = len(summaries_tokens)\n",
    "\n",
    "def construct_positional_indexes(corpus):\n",
    "    positional_index = defaultdict(lambda: defaultdict(lambda: {'title': [], 'abstract': []}))\n",
    "    for  data in corpus:\n",
    "        title, summaries, paper_id = data['title'], data['abstract'], data['id']\n",
    "        for position, token in enumerate(title):\n",
    "            positional_index[token][paper_id]['title'].append(position)\n",
    "        for position, token in enumerate(summaries):\n",
    "            positional_index[token][paper_id]['abstract'].append(position)\n",
    "\n",
    "    return positional_index\n",
    "positional_index = construct_positional_indexes(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "cmnjLtSZhay7"
   },
   "outputs": [],
   "source": [
    "number_of_documents = len(corpus)\n",
    "def get_idf(token, prob=False):\n",
    "    df = len(positional_index[token])\n",
    "    if prob:\n",
    "        return max(0, math.log10((number_of_documents - df) / (df + 1)))\n",
    "    else:\n",
    "        return math.log10(number_of_documents / (df + 1))\n",
    "\n",
    "def get_logarithmic_tf(tf_raw):\n",
    "    if tf_raw == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 + math.log10(tf_raw)\n",
    "\n",
    "def get_doc_scores(tf_query, method, field='title'):\n",
    "    doc_scores = defaultdict(int)\n",
    "    norm_factors = defaultdict(int)\n",
    "\n",
    "    doc_lengths = title_lengths if field == 'title' else summaries_lengths\n",
    "    average_doc_length = sum([v for v in doc_lengths.values()]) / len(doc_lengths)\n",
    "    for token, tf_query_raw in tf_query.items():\n",
    "        if token not in positional_index:\n",
    "            continue\n",
    "        for doc_id, positions in positional_index[token].items():\n",
    "\n",
    "            if len(positions[field]) == 0:\n",
    "                continue\n",
    "            if method == 'ltn-lnn':\n",
    "                tf_doc_raw = len(positions.get(field, []))\n",
    "                tf_doc = get_logarithmic_tf(tf_doc_raw)\n",
    "                idf = get_idf(token)\n",
    "                doc_coeff = idf * tf_doc\n",
    "\n",
    "                tf_query = get_logarithmic_tf(tf_query_raw)\n",
    "                doc_scores[doc_id] += tf_query * doc_coeff\n",
    "\n",
    "            if method == 'ltc-lnc':\n",
    "                tf_doc_raw = len(positions.get(field, []))\n",
    "                tf_doc = get_logarithmic_tf(tf_doc_raw)\n",
    "                idf = get_idf(token)\n",
    "                doc_coeff = idf * tf_doc\n",
    "                tf_query = get_logarithmic_tf(tf_query_raw)\n",
    "\n",
    "                doc_scores[doc_id] += tf_query * doc_coeff\n",
    "                norm_factors[doc_id] += doc_coeff ** 2\n",
    "            if method == 'okapi25':\n",
    "                k1 = 1.5\n",
    "                b = 0.75\n",
    "                tf_token_doc = len(positions.get(field, []))\n",
    "                doc_scores[doc_id] += get_idf(token) * (k1 + 1) * tf_token_doc / (k1 * (1 - b + b * doc_lengths[doc_id] / average_doc_length) + tf_token_doc)\n",
    "\n",
    "    if method == 'ltc-lnc':\n",
    "        for doc_id in doc_scores:\n",
    "            doc_scores[doc_id] /= math.sqrt(norm_factors[doc_id])\n",
    "\n",
    "    return doc_scores\n",
    "\n",
    "def print_result(doc_ids, title_query, abstract_query):\n",
    "    corrected_title_query = preprocess_text(title_query)\n",
    "    abstract_query = preprocess_text(abstract_query)\n",
    "\n",
    "    print('________________________________')\n",
    "    for doc_id in doc_ids:\n",
    "        row = pdf[pdf['id'] == doc_id]\n",
    "        title_content =row['titles'].to_string()\n",
    "        abstract_content =row['abstracts'].to_string()\n",
    "        print(f\"Paper Id: {doc_id}\")\n",
    "        print(f'Title Snippet: {title_content}')\n",
    "        print(f'Abstract Snippet: {abstract_content}')\n",
    "        print('________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Q7eLAMgkggb2"
   },
   "outputs": [],
   "source": [
    "def search(title_query: str, abstract_query: str, max_result_count: int, method: str = 'ltn-lnn', weight: float = 0.5,\n",
    "           print=False, preferred_field: str = None):\n",
    "    \"\"\"\n",
    "        Finds relevant documents to query\n",
    "\n",
    "        Parameters\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        max_result_count: Return top 'max_result_count' docs which have the highest scores.\n",
    "                          notice that if max_result_count = -1, then you have to return all docs\n",
    "\n",
    "        mode: 'detailed' for searching in title and text separately.\n",
    "              'overall' for all words, and weighted by where the word appears on.\n",
    "\n",
    "        where: when mode ='detailed', when we want search query\n",
    "                in title or text not both of them at the same time.\n",
    "\n",
    "        method: 'ltn-lnn' or 'ltc-lnc' or 'okapi25'\n",
    "\n",
    "        preferred_field: A list containing preference rate to Dr. Rabiee, Dr. Soleymani, Dr. Rohban,\n",
    "                         Dr. Kasaei, and Dr. Sharifi's papers, respectively.\n",
    "\n",
    "        Returns\n",
    "        ----------------------------------------------------------------------------------------------------\n",
    "        list\n",
    "        Retrieved documents with snippet\n",
    "    \"\"\"\n",
    "\n",
    "    tf_query_title = defaultdict(int)\n",
    "    tf_query_abstract = defaultdict(int)\n",
    "\n",
    "    tokens_title = preprocess_text(title_query) if title_query else []\n",
    "    tokens_abstract = preprocess_text(abstract_query) if abstract_query else []\n",
    "    query_tokens = tokens_title + tokens_abstract\n",
    "\n",
    "    scores = collections.defaultdict(list)\n",
    "    for token in tokens_title:\n",
    "        tf_query_title[token] += 1\n",
    "\n",
    "    for token in tokens_abstract:\n",
    "        tf_query_abstract[token] += 1\n",
    "\n",
    "    title_query_doc_scores = get_doc_scores(tf_query_title, method, 'title')\n",
    "    abstract_query_doc_scores = get_doc_scores(tf_query_abstract, method, 'abstract')\n",
    "\n",
    "    total_doc_ids = list(set(list(title_query_doc_scores.keys()) + list(abstract_query_doc_scores.keys())))\n",
    "    doc_scores = collections.defaultdict(int)\n",
    "\n",
    "    for doc_id in total_doc_ids:\n",
    "        title_score = title_query_doc_scores.get(doc_id, 0)\n",
    "        abstract_score = abstract_query_doc_scores.get(doc_id, 0)\n",
    "\n",
    "        doc_scores[doc_id] = weight * title_score + (1 - weight) * abstract_score\n",
    "        if preferred_field is not None:\n",
    "            doc_scores[doc_id] = doc_scores[doc_id] * preferred_field.get(id_prof_mapping[doc_id], 0)\n",
    "\n",
    "    sorted_doc_ids = [key for key, value in sorted(doc_scores.items(), key=lambda item: item[1], reverse=True)]\n",
    "    applicable_doc_ids = sorted_doc_ids[:max_result_count]\n",
    "    if print:\n",
    "        print_result(applicable_doc_ids, title_query, abstract_query)\n",
    "    return applicable_doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qlbuw2tehJ_d",
    "outputId": "d08c273e-7775-4047-b560-9a6865c11bed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________\n",
      "Paper Id: bb2afd8172469fef7276e9789b306e085ed6e650\n",
      "Title Snippet: 837    Real-time Inference in Multi-sentence Tasks wi...\n",
      "Abstract Snippet: 837    The use of deep pretrained bidirectional trans...\n",
      "________________________________\n",
      "Paper Id: c7fc1cac162c0e2a934704184c7554fd6b6253f0\n",
      "Title Snippet: 1011    Pretrained Encyclopedia: Weakly Supervised Kno...\n",
      "Abstract Snippet: 1011    Recent breakthroughs of pretrained language mo...\n",
      "________________________________\n",
      "Paper Id: c6499788267a24b8616f7ea444fc91577160bf25\n",
      "Title Snippet: 1208    C L ] 4 F eb 2 02 1 Knowledge-Aware Language M...\n",
      "Abstract Snippet: 1208    How much knowledge do pretrained language mode...\n",
      "________________________________\n",
      "Paper Id: 011cdf321549ec4775618357f907fa36902d455d\n",
      "Title Snippet: 1808    PRETRAIN KNOWLEDGE-AWARE LANGUAGE MODELS\n",
      "Abstract Snippet: 1808    How much knowledge do pretrained language mode...\n",
      "________________________________\n",
      "Paper Id: 6c8503803760c5c7790f72437d0f8b874334e6f0\n",
      "Title Snippet: 779    Span Selection Pre-training for Question Answe...\n",
      "Abstract Snippet: 779    BERT (Bidirectional Encoder Representations fr...\n",
      "________________________________\n",
      "Paper Id: 5e1621967c6a85bfa2dc0277a09bd0d2d9789e47\n",
      "Title Snippet: 1086    Few-shot Knowledge Graph-to-Text Generation wi...\n",
      "Abstract Snippet: 1086    This paper studies how to automatically genera...\n",
      "________________________________\n",
      "Paper Id: 51b9492f98ca667c098a0d54ef5d1c3f32455ee6\n",
      "Title Snippet: 1240    K-LM: Knowledge Augmenting in Language Models ...\n",
      "Abstract Snippet: 1240    The use of superior algorithms and complex arc...\n",
      "________________________________\n",
      "Paper Id: 6ff72eb17d32af6bbcdb17421fbc6d28e373b05d\n",
      "Title Snippet: 1126    KGNER: Improving Chinese Named Entity Recognit...\n",
      "Abstract Snippet: 1126    Recently, the lexicon method has been proven t...\n",
      "________________________________\n",
      "Paper Id: 2a50f79127e01f22cbaa5b5ae92adc6a9fd8e644\n",
      "Title Snippet: 530    KGDetector: Detecting Chinese Sensitive Inform...\n",
      "Abstract Snippet: 530    The Bidirectional Encoder Representations from...\n",
      "________________________________\n",
      "Paper Id: df2b0e26d0599ce3e70df8a9da02e51594e0e992\n",
      "Title Snippet: 777    BERT: Pre-training of Deep Bidirectional Trans...\n",
      "Abstract Snippet: 777    We introduce a new language representation mod...\n",
      "________________________________\n"
     ]
    }
   ],
   "source": [
    "query = \"pretrained transformers\"\n",
    "result = search(query, query, 10, method='ltn-lnn', print=True, preferred_field={\n",
    "    \"SOLEYMANI\":4,\n",
    "    \"ROHBAN\":3,\n",
    "    \"RABIEE\": 1,\n",
    "    'KASAEE': 2,\n",
    "    \"SHARIFI\": 2,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRntXIsVmObr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\" style=\"text-align: justify\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>رتبه‌بندی نویسندگان (۲۵ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>  \n",
    "    برای رتبه‌بندی نویسندگان، مفهوم ارجاع نویسندگان به یکدیگر مطرح می‌شود. زمانی که نویسنده A در مقاله خود به مقاله P که نویسنده B جزو نویسندگان آن مقاله یعنی مقاله P می‌باشد، ارجاع دهد، می‌گوییم که نویسنده A به نویسنده B ارجاع داده است. با توجه به این رابطه، می‌توان گراف ارجاعات بین نویسندگان را ایجاد و سپس با استفاده از الگوریتم HITS\n",
    "نویسندگان را رتبه‌بندی کرد. برای رتبه‌بندی نیاز است تا از شاخص‌های hub و authority استفاده کنیم.\n",
    "\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e06-W-LZmObt",
    "outputId": "50bb90fa-e50a-4e2b-b29d-5d58336ffa4a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "professor: SOLEYMANI, top_authors: ['G. Litjens', 'M. Balkenhol', 'B. van Ginneken', 'J. A. van der Laak', 'M. Hermsen']\n",
      "professor: ROHBAN, top_authors: ['Anne E Carpenter', 'Shantanu Singh', 'B. Cimini', 'M. Kost-Alimova', 'M. Rohban']\n",
      "professor: RABIEE, top_authors: ['Chunhua Shen', 'Hao Chen', 'Hanxi Li', 'Jing Wu', 'Mingwen Wang']\n",
      "professor: KASAEE, top_authors: ['Jiri Matas', 'Dong Wang', 'Martin Danelljan', 'M. Felsberg', 'Huchuan Lu']\n",
      "professor: SHARIFI, top_authors: ['H. Baharvand', 'S. Moradi', 'A. Sharifi-Zarchi', 'T. Braun', 'Sepideh Mollamohammadi']\n"
     ]
    }
   ],
   "source": [
    "def hit_algorithm(papers, n):\n",
    "    \"\"\"\n",
    "        Implementing the HITS algorithm to score authors based on their papers and co-authors.\n",
    "\n",
    "        Parameters\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        papers: A list of paper dictionaries with the following keys:\n",
    "                \"id\": A unique ID for the paper\n",
    "                \"title\": The title of the paper\n",
    "                \"abstract\": The abstract of the paper\n",
    "                \"date\": The year in which the paper was published\n",
    "                \"authors\": A list of the names of the authors of the paper\n",
    "                \"related_topics\": A list of IDs for related topics (optional)\n",
    "                \"citation_count\": The number of times the paper has been cited (optional)\n",
    "                \"reference_count\": The number of references in the paper (optional)\n",
    "                \"references\": A list of IDs for papers that are cited in the paper (optional)\n",
    "        n: An integer representing the number of top authors to return.\n",
    "\n",
    "        Returns\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        List\n",
    "        list of the top n authors based on their hub scores.\n",
    "    \"\"\"\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    for paper in papers:\n",
    "        authors, referenced_authors = paper['authors'], []\n",
    "        references = paper['references']\n",
    "        for reference in references:\n",
    "            reference_paper = next((p for p in papers if p['id'] == reference), None)\n",
    "            if reference_paper:\n",
    "                referenced_authors += reference_paper['authors']\n",
    "\n",
    "        for author in authors:\n",
    "            for referenced_author in referenced_authors:\n",
    "                G.add_edge(author, referenced_author)\n",
    "\n",
    "\n",
    "    hubs = nx.hits(G)[0]\n",
    "    return sorted(hubs, key=hubs.get, reverse=True)[:n]\n",
    "\n",
    "\n",
    "for prof_name, data in data_dict.items():\n",
    "    print(f'professor: {prof_name}, top_authors: {hit_algorithm(data, 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56oBaPhfmObv",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>سیستم پیشنهادگر (۲۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این بخش سعی می‌کنیم که یک سیستم پیشنهادگر مقالات بر اساس جست‌و‌جو‌ها یا علايق یک کاربر پیاده‌سازی کنیم، سیستم پیشنهاد دهنده‌ای که قصد داریم آن را ایجاد کنیم،‌ باید بتواند بر اساس لیستی از مقالاتی که کاربر قبلا آن‌ها را مطالعه کرده یا به آن‌ها علاقه داشته است، مقالات تازه انتشار یافته‌‌ی جدید را به کاربر پیشنهاد دهد.\n",
    "\n",
    "در فایل recommended_papers.json\n",
    "لیستی از کاربران قرار دارد که در فیلد positive_papers هر کاربر،\n",
    "تعداد ۵۰ مقاله از مقالاتی که کاربر به آن‌ها علاقه داشته است مشخص شده است. و همچینین در فیلد recommendedPapers هر کاربر تعداد ۱۰ مقاله به ترتیب اهمیت، از مقالات جدیدی که کاربر آن‌ها را پسندیده است قرار دارد.\n",
    "\n",
    "در این بخش هدف شما یادگیری سیستم پیشنهاد‌ دهنده بر اساس همین داده‌ها می‌باشد، و به عبارتی شما بایستی کاربر‌ها را به دو دسته آموزش و آزمایش تقسیم کنید، و بر اساس داده‌های آموزشی بتوانید مقالات جدید مورد پسند کاربرهای آزمایش را پیش‌بینی کنید. (بنابراین در این پیش‌بینی نمی‌توانید از فیلد recommendedPapers این کاربران استفاده کنید.)\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "TWZLT5zImObw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('recommended_papers.json', 'r') as fp:\n",
    "    recommended_papers = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "NJLrGCkfmOby",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_user = recommended_papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eX3DWvt9mObz",
    "outputId": "a10dac8c-4c96-4b41-ea67-63bdaef28adc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d9404b4a794c07b5e2cdf3203aabf06d70c6be9b\n",
      "CENTAURO: A Hybrid Locomotion and High Power Resilient Manipulation Platform\n",
      "Despite the development of a large number of mobile manipulation robots, very few platforms can demonstrate the required strength and mechanical sturdiness to accommodate the needs of real-world applications with high payload and moderate/harsh physical interaction demands, e.g., in disaster-response scenarios or heavy logistics/collaborative tasks. In this letter, we introduce the design of a wheeled-legged mobile manipulation platform capable of executing demanding manipulation tasks, and demonstrating significant physical resilience while possessing a body size (height/width) and weight compatible to that of a human. The achieved performance is the result of combining a number of design and implementation principles related to the actuation system, the integration of body structure and actuation, and the wheeled-legged mobility concept. These design principles are discussed, and the solutions adopted for various robot components are detailed. Finally, the robot performance is demonstrated in a set of experiments validating its power and strength capability when manipulating heavy payload and executing tasks involving high impact physical interactions.\n",
      "['Computer Science']\n"
     ]
    }
   ],
   "source": [
    "print(sample_user['positive_papers'][0]['paperId'])\n",
    "print(sample_user['positive_papers'][0]['title'])\n",
    "print(sample_user['positive_papers'][0]['abstract'])\n",
    "print(sample_user['positive_papers'][0]['fieldsOfStudy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0-R_rBHtmOb3",
    "outputId": "c17cfab0-1536-4c3d-948b-f65dfe08a6fb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94eebbefe8a37cf394be899b85af295c2e3a1f01\n",
      "Efficient Parametric Approximations of Neural Network Function Space Distance\n",
      "It is often useful to compactly summarize important properties of model parameters and training data so that they can be used later without storing and/or iterating over the entire dataset. As a specific case, we consider estimating the Function Space Distance (FSD) over a training set, i.e. the average discrepancy between the outputs of two neural networks. We propose a Linearized Activation Function TRick (LAFTR) and derive an efficient approximation to FSD for ReLU neural networks. The key idea is to approximate the architecture as a linear network with stochastic gating. Despite requiring only one parameter per unit of the network, our approach outcompetes other parametric approximations with larger memory requirements. Applied to continual learning, our parametric approximation is competitive with state-of-the-art nonparametric approximations, which require storing many training examples. Furthermore, we show its efficacy in estimating influence functions accurately and detecting mislabeled examples without expensive iterations over the entire dataset.\n",
      "['Computer Science', 'Mathematics']\n"
     ]
    }
   ],
   "source": [
    "print(sample_user['recommendedPapers'][0]['paperId'])\n",
    "print(sample_user['recommendedPapers'][0]['title'])\n",
    "print(sample_user['recommendedPapers'][0]['abstract'])\n",
    "print(sample_user['recommendedPapers'][0]['fieldsOfStudy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AzeyyH-mOb5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h2>\n",
    "    <b>روش Collaborative Filtering (۱۰ نمره)</b>\n",
    "    </h2>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این راهکار سعی می‌کنیم با استفاده از کاربران مشابه با یک کاربر، سلیقه‌ی او را حدس بزنیم و مقالاتی را که کاربران مشابه دیده‌اند را به کاربر نمایش دهیم.\n",
    "\n",
    "در این روش ابتدا باید $N$ کاربر که سلیقه‌ی مشابه با کاربر $x$ دارند را پیدا کنید، و با ترکیب لیست مقالات جدید مورد علاقه‌ی آن $N$ کاربر مشابه،\n",
    " ۱۰ مقاله‌ به کاربر $x$ پیشنهاد دهید.\n",
    "\n",
    "توجه داشته باشید که برای اینکه شباهت دو کاربر را پیدا کنید، باید cosine_similarity بین بردار زمینه‌های مورد علاقه‌ی دو کاربر استفاده کنید. این بردار از $M$ درایه تشکیل شده است، که $M$ تعداد زمینه‌های یکتاییست که در داده‌ها وجود دارد. و در این بردار درایه‌ی $j$ام\n",
    "نشان دهنده‌ی نسبت تعداد مقالات خوانده‌ی شده‌ کاربر در زمینه‌ی $j$ به تعداد کل مقاله‌های خوانده شده توسط او می‌باشد. (توجه کنید که هر مقاله می‌تواند چند زمینه داشته باشد و بنابراین حاصل جمع درایه‌های این بردار الزاما یک نمی‌باشد)\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5c1P17mxrbq",
    "outputId": "e025b97a-aa74-4dc7-c9a4-d9d84423a551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433\n",
      "109\n"
     ]
    }
   ],
   "source": [
    "train_recommended_papers, validation_recommended_papers = train_test_split(recommended_papers, test_size=0.2, random_state=42)\n",
    "print(len(train_recommended_papers))\n",
    "print(len(validation_recommended_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "1mHgOnumyscs"
   },
   "outputs": [],
   "source": [
    "def get_unique_field_of_studies():\n",
    "    field_of_studies = []\n",
    "    for user in recommended_papers:\n",
    "        for paper in user['positive_papers']:\n",
    "            if 'fieldsOfStudy' in paper and paper['fieldsOfStudy'] is not None:\n",
    "                for field in paper.get('fieldsOfStudy', []):\n",
    "                      field_of_studies.append(field)\n",
    "\n",
    "    return list(set(field_of_studies))\n",
    "\n",
    "field_of_studies = get_unique_field_of_studies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "jj3BUACx0nIo"
   },
   "outputs": [],
   "source": [
    "def project_to_vector_space(recom_papers):\n",
    "    field_of_studies_cnt = len(field_of_studies)\n",
    "    vectors = []\n",
    "    for user in recom_papers:\n",
    "        vector = np.zeros(field_of_studies_cnt)\n",
    "        paper_vectors = np.zeros(field_of_studies_cnt)\n",
    "        for paper in user['positive_papers']:\n",
    "            if 'fieldsOfStudy' in paper and paper['fieldsOfStudy'] is not None:\n",
    "                for field in paper.get('fieldsOfStudy', []):\n",
    "                    paper_vectors[field_of_studies.index(field)] += 1\n",
    "        vector = np.divide(paper_vectors, len(user['positive_papers']))\n",
    "        vectors.append(vector)\n",
    "    return vectors\n",
    "\n",
    "train_projection = project_to_vector_space(train_recommended_papers)\n",
    "validation_projection = project_to_vector_space(validation_recommended_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "JiL-JnSk1_J_"
   },
   "outputs": [],
   "source": [
    "def get_near_users(user, users, n=10):\n",
    "    similarities = [(index, cosine_similarity([user], [user])) for index, user in enumerate(users)]\n",
    "    n_nearest = sorted(similarities, key=lambda x: x[1], reverse=True)[:n]\n",
    "    return n_nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "nhiRmId-mOb6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collaborative_filtering(user_id: int, N=10):\n",
    "    \"\"\"\n",
    "    Returns the top 10 related articles to the user, based on similar users (Similar users should be on \"train data\").\n",
    "\n",
    "    Parameters:\n",
    "    user_id (int): The unique index of the user.\n",
    "    N: The number of hyperparameter N in Nearest Neighbor algorithm.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of 10 article IDs that are most relevant to the user's interests.\n",
    "    \"\"\"\n",
    "    user = validation_projection[user_id]\n",
    "    n_nearest = get_near_users(user, train_projection, n=N)\n",
    "    user_rel_papers = [paper['paperId'] for nearest in n_nearest for paper in train_recommended_papers[nearest[0]]['recommendedPapers']]\n",
    "    user_rel_papers = list(set(user_rel_papers))\n",
    "    return user_rel_papers[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kjRV-U9V4aQZ",
    "outputId": "cdfba2c0-59b8-4a6f-a558-f12dd4d9a441"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7bd7539abdc3d696d75213d4011327218f79ce21',\n",
       " 'efeaa77602c13e521e8359d2dad21228cac435f6',\n",
       " '66fa8acfbf6dc1022d5aa2ee43fad20cda231f98',\n",
       " 'be557adf001ab189e6aa38264c53f8a79a1586a3',\n",
       " '62f3ecee1135503bb2cab776e915281521ef2f3a',\n",
       " '6c194ca1bcee1fab2dfb289dadcdd9a829b736df',\n",
       " '33320785835ba8cdd717cb8d043e99e942e0b491',\n",
       " '0dd2dafa9389f83160a63be3fde23b5d121a4786',\n",
       " 'b65dfaf9b95b21840848b3b77bb4df655305ac89',\n",
       " '17ec33c333867e6f40a689578f9f4244efd3bf76',\n",
       " 'b42734a9ef73e4b6064ba89e92e2796d1e932285',\n",
       " 'eab5282e937a9b09b3af8c0956f1aa09ff20cfa9',\n",
       " '52ee74d432d2d125937a86cb1b06757cb750bc3a',\n",
       " '79f43d149cd569abf46428ed8a27a8a2b3e44a8f',\n",
       " '00474bfec99c5b6852de1ac5190a1243b7a5fa91',\n",
       " '87f95ab94bb19eb7716e0d4bbe0d37ca30f64157',\n",
       " 'fbb4110c5c1940dbaebb60234a633448684f1b5b',\n",
       " '5848855493fd8e3ed69f8063c350c76f0e058734',\n",
       " '3256e193a308e451c7107e51bb96c3e9b5bb6ae3',\n",
       " '25261eb53d5413f4d76a4519b917bdccad39c315',\n",
       " 'dd71c525097d93d825b8abf67ce99111de421605',\n",
       " '590b8f5e17424d1d4be560a0d2b1c665d8d3c7f8',\n",
       " '15b3523eecb9a4035324662bbb77d7d8b7185d5b',\n",
       " '94eebbefe8a37cf394be899b85af295c2e3a1f01',\n",
       " '5a6a2253ce2241861b30b4e8f2ce7c9891638b21',\n",
       " '7ead40d4cdcf83b2c53af30dda2a2dfbf4bb88a3',\n",
       " '4ef86160b00fbcd0e378f07358fae1296fee081a',\n",
       " '4effe5659446d5d414d04e49c33663af2abd903e',\n",
       " '89e049e37d5bb0b66929727e365c07b9382cbebb',\n",
       " '600e824cadf41e3543a4c0db22226aeb1578bce9',\n",
       " '36ef2340ee034f43cde94f845bddaf4460f86072',\n",
       " '16176c4916b1a21323870f551d7b1f8d960659c6',\n",
       " 'a23b8f072625d6481ead4c8b6193f01cdddd7fe0',\n",
       " '1d36e7ba19be5db9694ed256ea21dae5f753ede3',\n",
       " '5cbadc7545b5296a8b245be20c78f8b9b628973c',\n",
       " '74eacbfde4cf7541ec06f16026ab66e405dac21e',\n",
       " '7d03af1ccf5404e23bee02903b41850e88cc8590',\n",
       " '908bdaa3588ac073d06b4452ffd5fce7bd9af042',\n",
       " '4aef43dddbe2f5df801e96d6670f1eca06972175',\n",
       " 'c91dcdf1422648d1fb028288315df68c88e14899',\n",
       " '2714c11c0809d638e1e501831913671914407e5d',\n",
       " '0e7208e3e760947d5c7d5af27a9355887dd001d6',\n",
       " '633db3db97e68bff8ffb7f0f1601ec3f4ac9899e',\n",
       " '2bc78572ccf408e9c6e26385229e296ff6c2b169',\n",
       " '682ff0690c87a31c6bc148e53f56b5b494621d66',\n",
       " 'def191769aad63afe2ffc912db716a2b48206629',\n",
       " 'ba852c774c00894376bc20cdccb884b0dbe1196b',\n",
       " 'd36a632f95b45dc072fa0cbac7e494b758e1146a',\n",
       " '14e1b43c0a337c7c0d59cb04b720ed7d5acd14c1',\n",
       " '057df6d53a4b7f6314a1d1b8a9f0f3664bd5aa01',\n",
       " 'c196e56442c18127c30d494c68b0a8f5c3a333f6']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collaborative_filtering(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhOyw8k-mOb7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h2>\n",
    "    <b>روش Content Based (۱۰ نمره)</b>\n",
    "    </h2>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این روش با استفاده از مقالات قبلی که کاربر آن‌ها را پسندیده است، به کاربر مقاله‌ی جدید پیشنهاد می‌دهیم.\n",
    "\n",
    "برای اینکار ابتدا تمام مقالات پیشنهاد شده برای تمام کاربرها را سر جمع کنید. (در واقع مدلی که پیاده‌سازی می‌کنید نباید بداند که به کدام کاربر چه مقالاتی پیشنهاد شده است)\n",
    "\n",
    "سپس بردار tf-idf برای تایتل هر یک از مقالات را ایجاد کنید، و میانگین بردار مقالات مورد علاقه‌ی هر فرد را با لیستی که از مقالات جدید سر جمع کردید مقایسه کنید و ۱۰ تا از شبیه‌ترین مقالات را خروجی دهید.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "c-TeX4GMmOb8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def content_based_recommendation(user_id):\n",
    "    \"\"\"\n",
    "    Returns the top 10 related articles to the user, based on the titles of the articles.\n",
    "\n",
    "    Parameters:\n",
    "    user_id (int): The unique index of the user.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of 10 article IDs that are most relevant to the user's interests.\n",
    "    \"\"\"\n",
    "    titles, paper_ids = [], []\n",
    "    test_user = validation_recommended_papers[user_id]\n",
    "    titles = [paper['title'] for paper in test_user['positive_papers']]\n",
    "    paper_ids = [paper['paperId'] for user in train_recommended_papers for paper in user['recommendedPapers']]\n",
    "    for user in train_recommended_papers:\n",
    "        for paper in user['recommendedPapers']:\n",
    "            titles.append(paper['title'])\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(titles)\n",
    "    titles_vectorized = vectorizer.transform(titles)\n",
    "    user_vector = np.array(np.mean(titles_vectorized[:len(user['positive_papers'])], axis=0))\n",
    "    similarities = cosine_similarity(user_vector, titles_vectorized[len(test_user['positive_papers']):]).ravel().tolist()\n",
    "    nearest_indices = np.argsort(similarities[0])[::-1]\n",
    "    n_nearest = []\n",
    "    for index in nearest_indices:\n",
    "        paper_id = paper_ids[index]\n",
    "        if paper_id not in n_nearest:\n",
    "            n_nearest.append(paper_id)\n",
    "\n",
    "    return n_nearest[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnKMRhJJmOb9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h2>\n",
    "    <b>ارزیابی سیستم‌های پیشنهادگر</b>\n",
    "    </h2>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این بخش سیستم‌های پیشنهادگری را که پیاده کرده‌اید را با استفاده از معیار nDCG و با استفاده از دادگان واقعی از علایق کاربران نسبت به مقالات جدید ارزیابی کنید و نتایج حاصل از دو روش را با هم مقایسه کنید.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "YsHd1HRb8jaL"
   },
   "outputs": [],
   "source": [
    "def get_DCG(rs):\n",
    "    score = sum((2 ** s - 1) / math.log2(i + 2) for i, s in enumerate(rs))\n",
    "    return score\n",
    "\n",
    "def get_NDCG(actuals: List[List[str]], predictions: List[List[str]]) -> float:\n",
    "    ndcg_score = 0.0\n",
    "    n = len(actuals)\n",
    "    for actual, prediction in zip(actuals, predictions):\n",
    "        rs = [1 if result in actual else 0 for result in prediction]\n",
    "        best_ranking = sorted(rs, reverse=True)\n",
    "        best_case = get_DCG(best_ranking)\n",
    "        dcg_score = get_DCG(rs)\n",
    "        ndcg_score += dcg_score / best_case if best_case != 0 else 0\n",
    "    mean_ndcg_score = ndcg_score / n\n",
    "    return mean_ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "b7d9oJuT8B98"
   },
   "outputs": [],
   "source": [
    "def evaluate(method, method_name):\n",
    "    preds, actuals = [], []\n",
    "    for index, user in enumerate(validation_recommended_papers):\n",
    "        predicted = method(index)\n",
    "        actual = [p['paperId'] for p in user['recommendedPapers']]\n",
    "        preds.append(predicted)\n",
    "        actuals.append(actual)\n",
    "\n",
    "    ndcg = get_NDCG(actuals, preds)\n",
    "    print(f'for {method_name}, NDCG = {round(ndcg, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5rkCHFB6mOb-",
    "outputId": "aa236d9a-b267-4f49-c8ab-b9fb230eae31",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for collaborative_filtering, NDCG = 0.435\n"
     ]
    }
   ],
   "source": [
    "evaluate(collaborative_filtering, 'collaborative_filtering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iJqGxLGy9oZW",
    "outputId": "d710bae2-b233-4e30-c0cb-7f4432e6bdf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for content_based_recommendation, NDCG = 0.0367\n"
     ]
    }
   ],
   "source": [
    "evaluate(content_based_recommendation, 'content_based_recommendation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukdm0XjmmOb_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\" style=\"text-align: justify\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>رابط کاربری (تا ۱۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این بخش\n",
    " باید یک واسط کاربری ساده برای اجرای تعاملی بخش‌های مختلف سیستم که از فاز ۱ ساخته‌اید و همچنین مشاهده نتایج پیاده‌سازی کنید. در صورت پیاده سازی زیبا و بهتر رابط کاربری تا ده نمره نمره امتیازی نیز در نظر گرفته خواهد شد.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "ogJMg6MAOuo4"
   },
   "outputs": [],
   "source": [
    "prof_list = [\n",
    "    \"SOLEYMANI\",\n",
    "    \"ROHBAN\",\n",
    "    \"RABIEE\",\n",
    "    \"KASAEE\",\n",
    "    \"SHARIFI\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vvz1xFWimOcA",
    "outputId": "8ab515b8-6c50-4ad9-d3f5-b27ccd6dc1fe",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter your query for the title: bidirectional transformers\n",
      "enter your query for the abstract: bidirectional transformers\n",
      "do you have preferences on the professors? Yes/No No\n",
      "\n",
      "here are the results:\n",
      "\n",
      "________________________________\n",
      "Paper Id: df2b0e26d0599ce3e70df8a9da02e51594e0e992\n",
      "Title Snippet: 777    BERT: Pre-training of Deep Bidirectional Trans...\n",
      "Abstract Snippet: 777    We introduce a new language representation mod...\n",
      "________________________________\n",
      "Paper Id: c79a8fd667f59e6f1ca9d54afc34f792e9079c7e\n",
      "Title Snippet: 982    TRANS-BLSTM: Transformer with Bidirectional LS...\n",
      "Abstract Snippet: 982    Bidirectional Encoder Representations from Tra...\n",
      "________________________________\n",
      "Paper Id: 224060b48c1576e34ba9a7ca28424cadd9d27318\n",
      "Title Snippet: 991    Extending Answer Prediction for Deep Bi-direct...\n",
      "Abstract Snippet: 991    The current state-of-the-art technique used to...\n",
      "________________________________\n",
      "Paper Id: 2c4cd2b456ab8e4f1e6195f5eb6954eb084060ec\n",
      "Title Snippet: 1119    Utilizing Bidirectional Encoder Representation...\n",
      "Abstract Snippet: 1119    Pre-training a transformer-based model for the...\n",
      "________________________________\n",
      "Paper Id: bb2afd8172469fef7276e9789b306e085ed6e650\n",
      "Title Snippet: 837    Real-time Inference in Multi-sentence Tasks wi...\n",
      "Abstract Snippet: 837    The use of deep pretrained bidirectional trans...\n",
      "________________________________\n",
      "Paper Id: 24b8a0b02bcb7934967757fc59d273a71ba67e30\n",
      "Title Snippet: 1843    TransUNet: Transformers Make Strong Encoders f...\n",
      "Abstract Snippet: 1843    Medical image segmentation is an essential pre...\n",
      "________________________________\n",
      "Paper Id: 1cb5a1fce0b65b616e69cc5ffd4e43e03d259e97\n",
      "Title Snippet: 622    Transforming medical imaging with Transformers...\n",
      "Abstract Snippet: 622    Transformer, one of the latest technological a...\n",
      "________________________________\n",
      "Paper Id: ec561a375a100097f00fcf4924a2c14a9a7735e7\n",
      "Title Snippet: 568    Medical Image Segmentation using Squeeze-and-E...\n",
      "Abstract Snippet: 568    Medical image segmentation is important for co...\n",
      "________________________________\n",
      "Paper Id: 6c8503803760c5c7790f72437d0f8b874334e6f0\n",
      "Title Snippet: 779    Span Selection Pre-training for Question Answe...\n",
      "Abstract Snippet: 779    BERT (Bidirectional Encoder Representations fr...\n",
      "________________________________\n",
      "Paper Id: 85231347d25a29041ae9a1322d6e865d37c1d138\n",
      "Title Snippet: 959    Pre-Training of Deep Bidirectional Protein Seq...\n",
      "Abstract Snippet: 959    Bridging the exponentially growing gap between...\n",
      "________________________________\n"
     ]
    }
   ],
   "source": [
    "title_query = input('enter your query for the title: ')\n",
    "abstract_query = input('enter your query for the abstract: ')\n",
    "preference = True if input('do you have preferences on the professors? Yes/No ') == 'Yes' else False\n",
    "\n",
    "preference_dict = None\n",
    "if preference:\n",
    "    preference_dict = dict()\n",
    "    for prof in prof_list:\n",
    "        prof_coeff = float(input(f'enter a number for the degree of preference for Prof.{prof}: '))\n",
    "        preference_dict[prof] = prof_coeff\n",
    "print()\n",
    "print('here are the results:')\n",
    "print()\n",
    "time.sleep(1.4)\n",
    "results = search(title_query, abstract_query, 10, method='ltn-lnn', print=True, preferred_field=preference_dict)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "08ac30a6a1fd2e576b33e03f7d61c3a285d7ee0582c2dd23dde6343ef303ebe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
